{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOQmGqG38yEDTkcc1UtBA0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmiMontalto/Analisis-V1/blob/main/%22FinBERT_para_Decisiones_de_Trading_Compra_y_Venta_Predictiva_a_partir_de_Datos_Hist%C3%B3ricos_de_Criptomonedas%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por supuesto, aquí está un resumen de los acuerdos y características para el desarrollo del modelo:\n",
        "\n",
        "---\n",
        "\n",
        "**Objetivo del Proyecto**:\n",
        "Desarrollar un modelo de inteligencia artificial basado en FinBERT para predecir, con un día de anticipación, si se debe comprar o vender una acción. La decisión es binaria: compra o venta. El modelo se entrenará utilizando datos históricos de precios de criptos y se validará mediante validación cruzada en el tiempo. La fucnion def detect_and_plot_peaks(data), marcara la correcta divicion de la informacion y la columna que da por resultado dara la informacion contra la cual contrastar la info.\n",
        "\n",
        "**Características del Modelo**:\n",
        "\n",
        "Para avanzar con el uso de FinBERT en tu proyecto de predicción de señales de compra o venta para acciones de criptomonedas, aquí está un esquema de alto nivel del proceso que podríamos seguir:\n",
        "\n",
        "### Paso 1: Preparación de Datos\n",
        "1. **Recopilación de Datos**: Asegúrate de que los datos históricos de las criptomonedas están completos y limpios.\n",
        "2. **Generación de Características**: Calcula los indicadores técnicos necesarios y estandariza cualquier dato numérico que se vaya a utilizar.\n",
        "3. **Conversión a Texto**: Transforma las características numéricas en descripciones textuales que FinBERT pueda entender, manteniendo un contexto financiero relevante.\n",
        "4. **Etiquetado de Datos**: Utiliza las funciones que detectan picos alcistas y bajistas para etiquetar los datos con señales de 'compra' o 'venta'.\n",
        "5. **División de Datos**: Prepara conjuntos de datos de entrenamiento, validación y prueba, respetando la estructura de series temporales.\n",
        "\n",
        "### Paso 2: Ajuste Fino del Modelo\n",
        "1. **Carga de FinBERT**: Carga el modelo FinBERT preentrenado.\n",
        "2. **Adaptación del Tokenizador**: Adapta el tokenizador de FinBERT para manejar cualquier formato especial que hayas diseñado en la preparación de datos.\n",
        "3. **Procesamiento de Entrada**: Convierte tus descripciones textuales en el formato de entrada que requiere FinBERT (tokens, máscaras de atención, etc.).\n",
        "4. **Configuración de Entrenamiento**: Establece la función de pérdida (por ejemplo, entropía cruzada para clasificación), el optimizador (Adam suele ser una buena elección) y cualquier otro hiperparámetro.\n",
        "5. **Entrenamiento**: Entrena FinBERT en tu conjunto de datos, utilizando técnicas como el entrenamiento por lotes y la validación cruzada en el tiempo.\n",
        "6. **Validación y Ajuste**: Evalúa el modelo en el conjunto de datos de validación y realiza los ajustes necesarios en los hiperparámetros.\n",
        "\n",
        "### Paso 3: Evaluación del Modelo\n",
        "1. **Evaluación de Rendimiento**: Usa el conjunto de datos de prueba para evaluar el rendimiento del modelo utilizando métricas como precisión, recall y F1-score.\n",
        "2. **Interpretación de Resultados**: Analiza los resultados para entender cómo el modelo está tomando decisiones y qué tan bien se correlacionan con las señales reales de trading.\n",
        "\n",
        "### Paso 4: Despliegue del Modelo\n",
        "1. **Selección del Modelo**: Elige el mejor modelo basado en las métricas de evaluación.\n",
        "2. **Guardado del Modelo**: Guarda el modelo y cualquier procesamiento asociado de datos para que pueda ser utilizado consistentemente.\n",
        "3. **Integración en la Estrategia de Trading**: Integra el modelo en un sistema de trading automatizado o un flujo de trabajo donde las señales pueden ser utilizadas para tomar decisiones de trading.\n",
        "\n",
        "### Paso 5: Monitoreo y Mantenimiento\n",
        "1. **Monitoreo Continuo**: Monitorea el rendimiento del modelo en el tiempo para asegurar que sigue siendo válido a medida que se recopilan nuevos datos.\n",
        "2. **Reentrenamiento Periódico**: Planifica reentrenar el modelo periódicamente o cuando surjan cambios significativos en el mercado.\n",
        "\n",
        "Este esquema es solo un punto de partida. Cada uno de estos pasos puede requerir un trabajo detallado y ajustes específicos para adaptarse a las particularidades de los datos y los requisitos de tu proyecto. ¿Hay algún paso específico en el que quieras profundizar o iniciar el proceso?\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "hqFjUXODMNrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnNTodVcfQFd",
        "outputId": "8801f63d-4e9d-40c3-916b-252cb7693ae7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ta) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ta) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ta) (1.16.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=02b1cd50ce74affeca213063580514bb5825ba4d4ebae70f3a243d31bd531f98\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/67/4f/8a9f252836e053e532c6587a3230bc72a4deb16b03a829610b\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF4R1TN1MTGc",
        "outputId": "bbab1c69-459b-4951-e7e7-e145df02c597"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers[torch])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, accelerate, transformers\n",
            "Successfully installed accelerate-0.24.1 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsG7iaelMVt8",
        "outputId": "ceb72345-5b24-42be-846a-7ea51e335bec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrDOnP5lbPo3",
        "outputId": "96f0ebdb-c0a8-491c-c39b-776b12186419"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import  drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ys3frEnTfQ3V",
        "outputId": "2c9b2235-4765-4481-df4b-ee0f8437b19f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks"
      ],
      "metadata": {
        "id": "XccLSot2bPwN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pX5PVcnALOVF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def unify_detect_label_peaks(data):\n",
        "    # Detectar picos alcistas y bajistas\n",
        "    bullish_peaks, _ = find_peaks(data['Close'], distance=10, prominence=1)\n",
        "    bearish_peaks, _ = find_peaks(-data['Close'], distance=10, prominence=1)\n",
        "\n",
        "    # Inicializar la señal con venta hasta que se encuentre el primer pico de compra\n",
        "    data['Numeric_Signal'] = -1  # Todos los puntos inicialmente a venta\n",
        "\n",
        "    # Ordenar y combinar los picos\n",
        "    all_peaks = np.sort(np.concatenate((bullish_peaks, bearish_peaks)))\n",
        "\n",
        "    # Verificar si se encontraron picos\n",
        "    if all_peaks.size > 0:\n",
        "        # Alternar señales entre los picos\n",
        "        signal = 1\n",
        "        for i in range(len(all_peaks)-1):\n",
        "            # Asignar señal de compra o venta entre los picos usando .loc para evitar SettingWithCopyWarning\n",
        "            data.loc[all_peaks[i]:all_peaks[i+1], 'Numeric_Signal'] = signal\n",
        "            signal *= -1  # Cambiar señal para el próximo segmento\n",
        "\n",
        "        # Asegurarse de que el último segmento después del último pico también tenga señal\n",
        "        if all_peaks[-1] < len(data) - 1:\n",
        "            data.loc[all_peaks[-1]:, 'Numeric_Signal'] = signal\n",
        "\n",
        "    # Mapear señales numéricas a señales de trading\n",
        "    data['Trade_Signal'] = data['Numeric_Signal'].map({1: 'compra', -1: 'venta'})\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "\n",
        "# Paso 1: Preprocesamiento de Datos\n",
        "\n",
        "def load_and_preprocess_data(crypto_list, data_folder):\n",
        "    all_data = pd.DataFrame()  # DataFrame para concatenar todos los datos\n",
        "\n",
        "    for crypto in crypto_list:\n",
        "        file_path = os.path.join(data_folder, f\"{crypto}_daily_normalized.csv\")\n",
        "        if os.path.exists(file_path):\n",
        "            df = pd.read_csv(file_path)\n",
        "            # Aquí puedes aplicar cualquier preprocesamiento adicional necesario\n",
        "\n",
        "            # Luego etiquetas los datos con la función unify_detect_label_peaks\n",
        "            # Asumimos que esta función ya está definida en el contexto\n",
        "            df = unify_detect_label_peaks(df)  # No necesitamos la trama aquí\n",
        "\n",
        "            # Concatenamos los datos de esta cripto con el DataFrame global\n",
        "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "        else:\n",
        "            print(f\"No se encontró el archivo para {crypto}\")\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Asumiendo que la función unify_detect_label_peaks ya está definida y disponible en el contexto\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JIKWF6racFOu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Paso 2: Carga y Ajuste Fino de FinBERT\n",
        "\n",
        "\n",
        "def fine_tune_finbert(preprocessed_data):\n",
        "    # Convertir las características numéricas en texto\n",
        "    preprocessed_data['text_data'] = preprocessed_data.apply(\n",
        "        lambda x: f\"ZScore {x['ZScore']} Close_21SMA_Ratio {x['Close_21SMA_Ratio']} OHLC_21SMA_Ratio {x['OHLC_21SMA_Ratio']} \"\n",
        "                  f\"rsi_2 {x['rsi_2']} RSI2_RSI14_Ratio {x['RSI2_RSI14_Ratio']} ADX {x['ADX']} ADX_pos {x['ADX_pos']} \"\n",
        "                  f\"ADX_neg {x['ADX_neg']} BB_Upper_Ratio {x['BB_Upper_Ratio']} BB_Lower_Ratio {x['BB_Lower_Ratio']} Diff {x['Diff']}\",\n",
        "        axis=1\n",
        "    )\n",
        "    # Suponiendo que preprocessed_data['text_data'] ya está correctamente formateado como una lista de strings\n",
        "    text_data = list(preprocessed_data['text_data'])\n",
        "\n",
        "    # Cargar el tokenizador de FinBERT\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "\n",
        "    # Tokenizar los datos de texto\n",
        "    tokens = tokenizer(\n",
        "    text_data,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        "    )\n",
        "    # Asegúrate de que 'tokens' es un diccionario\n",
        "    if not isinstance(tokens, dict):\n",
        "        raise ValueError(\"El tokenizador no está devolviendo un diccionario como se esperaba.\")\n",
        "\n",
        "    # Convertir las etiquetas en un formato adecuado para la clasificación binaria\n",
        "    labels = preprocessed_data['Numeric_Signal'].map({1: 1, -1: 0}).values\n",
        "\n",
        "    # Dividir los datos en conjuntos de entrenamiento y validación\n",
        "    train_data, validation_data, train_labels, validation_labels = train_test_split(\n",
        "        tokens['input_ids'], labels, test_size=0.1, random_state=42\n",
        "    )\n",
        "\n",
        "    # Asegurarse de que todos los datos son tensores\n",
        "    train_data = torch.tensor(train_data)\n",
        "    validation_data = torch.tensor(validation_data)\n",
        "    train_labels = torch.tensor(train_labels)\n",
        "    validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "    # Convertir los conjuntos de datos a formato de PyTorch\n",
        "    train_dataset = TensorDataset(train_data, train_labels)\n",
        "    valid_dataset = TensorDataset(validation_data, validation_labels)\n",
        "\n",
        "    # Cargar el modelo FinBERT preentrenado\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"yiyanghkust/finbert-tone\",\n",
        "        num_labels=2,  # Especificamos que solo tenemos 2 etiquetas para la clasificación\n",
        "        ignore_mismatched_sizes=True  # Agregamos este argumento para ignorar el desajuste de tamaño\n",
        "    )\n",
        "    # Argumentos de entrenamiento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        load_best_model_at_end=False,  # Cargar el mejor modelo al final del entrenamiento\n",
        "        evaluation_strategy=\"epoch\",  # Evaluación al final de cada época\n",
        "    )\n",
        "\n",
        "    # Instanciar Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=valid_dataset,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    trainer.train()\n",
        "\n",
        "    return model, tokenizer, trainer\n"
      ],
      "metadata": {
        "id": "QhgGSdCEcFRu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "# Paso 3: Entrenamiento y Evaluación\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "def train_and_evaluate(model, tokenizer, train_data, validation_data):\n",
        "    # Argumentos de entrenamiento\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=64,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        evaluation_strategy=\"epoch\",  # Evaluación al final de cada época\n",
        "        save_strategy=\"epoch\",        # Guardar al final de cada época para que coincida con evaluation_strategy\n",
        "        load_best_model_at_end=True   # Cargar el mejor modelo al final del entrenamiento\n",
        "    )\n",
        "\n",
        "    # Instanciar Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=validation_data,\n",
        "        compute_metrics=compute_metrics,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    # Entrenar el modelo\n",
        "    train_result = trainer.train()\n",
        "    trainer.save_model()  # Guarda el mejor modelo\n",
        "\n",
        "    # Evaluación\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    # Devuelve resultados\n",
        "    return train_result, metrics\n"
      ],
      "metadata": {
        "id": "BNM2oriRcFU_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, tokenizer, save_directory):\n",
        "    # Crear el directorio si no existe\n",
        "    if not os.path.exists(save_directory):\n",
        "        os.makedirs(save_directory)\n",
        "\n",
        "    # Guardar el modelo\n",
        "    model_save_path = os.path.join(save_directory, \"finbert_model\")\n",
        "    model.save_pretrained(model_save_path)\n",
        "\n",
        "    # Guardar el tokenizador\n",
        "    tokenizer_save_path = os.path.join(save_directory, \"finbert_tokenizer\")\n",
        "    tokenizer.save_pretrained(tokenizer_save_path)\n",
        "\n",
        "    print(f\"Modelo guardado en {model_save_path}\")\n",
        "    print(f\"Tokenizador guardado en {tokenizer_save_path}\")\n"
      ],
      "metadata": {
        "id": "PKPOX4W7ekfK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Ejecución del flujo de trabajo\n",
        "data_folder = \"/content/drive/MyDrive/Bot 3.0/Historicos de cotizacion/Datos normalizados\"\n",
        "crypto_list = [\n",
        "    'BTC', 'ETH', 'XRP', 'BCH', 'LTC', 'EOS', 'BNB',\n",
        "    'XLM', 'TRX', 'ADA', 'XTZ', 'XMR', 'DASH', 'ZEC',\n",
        "    'ETC', 'NEO', 'MKR', 'XEM', 'DOGE', 'VET'\n",
        "]\n",
        "\n",
        "preprocessed_data = load_and_preprocess_data(crypto_list, data_folder)\n",
        "finbert_model = fine_tune_finbert(preprocessed_data)\n",
        "train_and_evaluate(finbert_model, preprocessed_data['train'], preprocessed_data['validation'])\n",
        "save_model(finbert_model, '/content/drive/MyDrive/Bot 3.0/Inteligencia artificial/Compra Venta desde picos/Transformer')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "MJwMbeIWeknR",
        "outputId": "cea54728-5067-48ab-aee9-d46f706814fd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-263459aa2bff>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpreprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrypto_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfinbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tune_finbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Bot 3.0/Inteligencia artificial/Compra Venta desde picos/Transformer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d3b0c9bf4cc0>\u001b[0m in \u001b[0;36mfine_tune_finbert\u001b[0;34m(preprocessed_data)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Asegúrate de que 'tokens' es un diccionario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El tokenizador no está devolviendo un diccionario como se esperaba.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Convertir las etiquetas en un formato adecuado para la clasificación binaria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: El tokenizador no está devolviendo un diccionario como se esperaba."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xPRHVvEjKBQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S5UWwqSgKBTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar la nueva función de detección y etiquetado de picos\n",
        "preprocessed_data = unify_detect_label_peaks(preprocessed_data)\n",
        "\n",
        "# Verificar las etiquetas\n",
        "label_counts = preprocessed_data['Numeric_Signal'].value_counts()\n",
        "print(\"Distribución de las etiquetas numéricas:\")\n",
        "print(label_counts)\n",
        "\n",
        "# Verificar que solo haya dos clases únicas\n",
        "unique_labels = preprocessed_data['Numeric_Signal'].unique()\n",
        "print(\"Etiquetas únicas en los datos:\", unique_labels)\n"
      ],
      "metadata": {
        "id": "oY7O_5yMLwAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_labels(preprocessed_data):\n",
        "    # Verificar si las señales numéricas son binarias (-1, 1)\n",
        "    if preprocessed_data['Numeric_Signal'].isin([-1, 1]).all():\n",
        "        print(\"Todas las señales numéricas son binarias (-1 para venta, 1 para compra).\")\n",
        "    else:\n",
        "        print(\"Hay señales numéricas que no son binarias.\")\n",
        "\n",
        "    # Contar las señales de compra y venta\n",
        "    signal_counts = preprocessed_data['Numeric_Signal'].value_counts()\n",
        "    print(f\"Señales de compra (1): {signal_counts.get(1, 0)}\")\n",
        "    print(f\"Señales de venta (-1): {signal_counts.get(-1, 0)}\")\n",
        "\n",
        "    return signal_counts\n",
        "\n",
        "# Usamos la función en nuestros datos preprocesados\n",
        "signal_counts = check_labels(preprocessed_data)\n",
        "print(signal_counts)\n"
      ],
      "metadata": {
        "id": "vfR0E_FKKBWa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}